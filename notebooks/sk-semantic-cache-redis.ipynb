{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SK RAG Pattern Foundational Concepts - Redis Cache\n",
    "\n",
    "Learning objectives:\n",
    "\n",
    "- RAG pattern foundational concepts\n",
    "- Redis cache as a vector database\n",
    "\n",
    "## Redis setup\n",
    "\n",
    "### Running Redis in a local container:\n",
    "\n",
    "- docker pull `redis/redis-stack:latest`\n",
    "  - **Note:** this version of redis includes the `RedisSearch` module\n",
    "- Then execute: \n",
    "  - `docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest`\n",
    "\n",
    "Connection string:\n",
    "\n",
    "- `REDIS_CONN_STR=localhost`\n",
    "\n",
    "### Running from Azure Redis Cache\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Load required .NET packages and supporting constants, classes, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>dotenv.net, 3.1.3</span></li><li><span>Microsoft.SemanticKernel, 1.4.0</span></li><li><span>Microsoft.SemanticKernel.Connectors.Redis, 1.4.0-alpha</span></li><li><span>Microsoft.SemanticKernel.Core, 1.4.0</span></li><li><span>Microsoft.SemanticKernel.Plugins.Memory, 1.4.0-alpha</span></li><li><span>StackExchange.Redis, 2.7.17</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 1.4.0\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Core, 1.4.0\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Plugins.Memory, 1.4.0-alpha\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Connectors.Redis, 1.4.0-alpha\"\n",
    "#r \"nuget: StackExchange.Redis\"\n",
    "#r \"nuget: dotenv.net\"\n",
    "\n",
    "using System;\n",
    "\n",
    "using System.IO;\n",
    "using System.Text;\n",
    "using System.Text.RegularExpressions;\n",
    "using System.Text.Json;\n",
    "using System.Text.Json.Serialization;\n",
    "using StackExchange.Redis;\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
    "using Microsoft.SemanticKernel.Connectors.Redis;\n",
    "using Microsoft.SemanticKernel.Memory;\n",
    "using Microsoft.SemanticKernel.Plugins.Memory;\n",
    "\n",
    "using dotenv.net;\n",
    "using InteractiveKernel = Microsoft.DotNet.Interactive.Kernel;\n",
    "\n",
    "#!import Models/Models.cs\n",
    "\n",
    "const int ADA_EMBEDDING_SIZE = 1536;\n",
    "const string MemoryCollectionName = \"SemanticCache\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the API Key and endpoints from environment variables or the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "// Load the .env file\n",
    "DotEnv.Load();\n",
    "\n",
    "// Get the OpenAI deployment name, endpoint, and key from the environment variables\n",
    "var deploymentName = Environment.GetEnvironmentVariable(\"GPT_OPENAI_DEPLOYMENT_NAME\");\n",
    "var endpoint = Environment.GetEnvironmentVariable(\"GPT_OPENAI_ENDPOINT\");\n",
    "var apiKey = Environment.GetEnvironmentVariable(\"GPT_OPENAI_KEY\");\n",
    "var redis_conn_str = Environment.GetEnvironmentVariable(\"REDIS_CONN_STR\");\n",
    "var adaDeploymentName = \"ada\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a kernel instance configured for text completions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "// I'm using a RAM stored Vector DB, but I can switch providers like Azure Search, DuckDB, SQLite, etc.\n",
    "#pragma warning disable CS8618,IDE0009,CA1051,CA1050,CA1707,CA2007,VSTHRD111,CS1591,RCS1110,CA5394,SKEXP0001,SKEXP0002,SKEXP0003,SKEXP0004,SKEXP0010,SKEXP0011,SKEXP0012,SKEXP0020,SKEXP0021,SKEXP0022,SKEXP0023,SKEXP0024,SKEXP0025,SKEXP0026,SKEXP0027,SKEXP0028,SKEXP0029,SKEXP0030,SKEXP0031,SKEXP0032,SKEXP0040,SKEXP0041,SKEXP0042,SKEXP0050,SKEXP0051,SKEXP0052,SKEXP0053,SKEXP0054,SKEXP0055,SKEXP0060,SKEXP0061,SKEXP0101,SKEXP0102\n",
    "\n",
    "var kernel = Kernel.CreateBuilder()\n",
    "    .AddAzureOpenAIChatCompletion(deploymentName, endpoint, apiKey)\n",
    "    .AddAzureOpenAITextEmbeddingGeneration(adaDeploymentName, endpoint, apiKey)\n",
    "    .Build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "#pragma warning disable CS8618,IDE0009,CA1051,CA1050,CA1707,CA2007,VSTHRD111,CS1591,RCS1110,CA5394,SKEXP0001,SKEXP0002,SKEXP0003,SKEXP0004,SKEXP0010,SKEXP0011,SKEXP0012,SKEXP0020,SKEXP0021,SKEXP0022,SKEXP0023,SKEXP0024,SKEXP0025,SKEXP0026,SKEXP0027,SKEXP0028,SKEXP0029,SKEXP0030,SKEXP0031,SKEXP0032,SKEXP0040,SKEXP0041,SKEXP0042,SKEXP0050,SKEXP0051,SKEXP0052,SKEXP0053,SKEXP0054,SKEXP0055,SKEXP0060,SKEXP0061,SKEXP0101,SKEXP0102\n",
    "\n",
    "ConnectionMultiplexer connectionMultiplexer = await ConnectionMultiplexer.ConnectAsync(redis_conn_str);\n",
    "IDatabase database = connectionMultiplexer.GetDatabase();\n",
    "var memoryStore = new RedisMemoryStore(database, vectorSize: ADA_EMBEDDING_SIZE);\n",
    "\n",
    "// Reset the collection\n",
    "if (await memoryStore.DoesCollectionExistAsync(MemoryCollectionName))\n",
    "    await memoryStore.DeleteCollectionAsync(MemoryCollectionName);\n",
    "\n",
    "var embeddingGenerator = new AzureOpenAITextEmbeddingGenerationService(adaDeploymentName, endpoint, apiKey);\n",
    "\n",
    "// The combination of the text embedding generator and the memory store makes up the 'SemanticTextMemory' object used to\n",
    "// store and retrieve memories.\n",
    "SemanticTextMemory textMemory = new(memoryStore, embeddingGenerator);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read and save to and from Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "async Task<Chunk?> CheckInCache(string question, double minRelevance=0.8)\n",
    "{\n",
    "    try \n",
    "    {\n",
    "        Chunk? chunk = null;    \n",
    "        #pragma warning disable CS8618,IDE0009,CA1051,CA1050,CA1707,CA2007,VSTHRD111,CS1591,RCS1110,CA5394,SKEXP0001,SKEXP0002,SKEXP0003,SKEXP0004,SKEXP0010,SKEXP0011,SKEXP0012,SKEXP0020,SKEXP0021,SKEXP0022,SKEXP0023,SKEXP0024,SKEXP0025,SKEXP0026,SKEXP0027,SKEXP0028,SKEXP0029,SKEXP0030,SKEXP0031,SKEXP0032,SKEXP0040,SKEXP0041,SKEXP0042,SKEXP0050,SKEXP0051,SKEXP0052,SKEXP0053,SKEXP0054,SKEXP0055,SKEXP0060,SKEXP0061,SKEXP0101,SKEXP0102\n",
    "        IAsyncEnumerable<MemoryQueryResult> queryResults =\n",
    "                        textMemory.SearchAsync(MemoryCollectionName, question, limit: 1, minRelevanceScore: minRelevance);\n",
    "        await foreach (MemoryQueryResult r in queryResults)\n",
    "        {\n",
    "            chunk = new Chunk(r.Metadata.Id, r.Metadata.Text, string.Empty);\n",
    "        }\n",
    "        return chunk;\n",
    "    }\n",
    "    catch (Exception)\n",
    "    {\n",
    "        return null;\n",
    "    }\n",
    "}\n",
    "\n",
    "async Task SaveInCache(Chunk chunk)\n",
    "{\n",
    "    await textMemory.SaveInformationAsync(MemoryCollectionName, id: chunk.Id, text: chunk.Text);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "async Task<string> CallLLM(string prompt, int maxTokens = 500, double temperature = 0.3)\n",
    "{\n",
    "    var skfunc = kernel.CreateFunctionFromPrompt(prompt, new OpenAIPromptExecutionSettings() { MaxTokens = maxTokens, Temperature = temperature, TopP = 1 });\n",
    "    var result = await kernel.InvokeAsync(skfunc);\n",
    "    return result.ToString();\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt the completion looking at the Cache results first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "async Task ProcessPrompt(string prompt)\n",
    "{\n",
    "    var chunk = await CheckInCache(prompt);\n",
    "    if (chunk is null)\n",
    "    {\n",
    "        // Handle Cache miss\n",
    "        Console.WriteLine($\"Cache miss:\\nuser: {prompt}\");\n",
    "        var result = await CallLLM(prompt);\n",
    "        Console.WriteLine($\"Calling the LLM\\nResult: {result}\");\n",
    "        chunk = new Chunk(Guid.NewGuid().ToString(), result, string.Empty);\n",
    "        Console.WriteLine($\"Adding the result to cache.\");\n",
    "        await SaveInCache(chunk);\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        Console.WriteLine($\"Cache hit:\");\n",
    "        Console.WriteLine($\"user: {prompt}\");\n",
    "        Console.WriteLine($\"assitant: {chunk.Text}\");\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache the results of the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss:\n",
      "user: What is the speed of light?\n",
      "Calling the LLM\n",
      "Result: The speed of light in a vacuum is approximately 299,792,458 meters per second (or about 186,282 miles per second). This is denoted by the symbol \"c\" in physics equations.\n",
      "Adding the result to cache.\n"
     ]
    }
   ],
   "source": [
    "await ProcessPrompt(\"What is the speed of light?\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Prompt slightly and try to get the answer from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache hit:\n",
      "user: State the speed of light.\n",
      "assitant: The speed of light in a vacuum is approximately 299,792,458 meters per second (or about 186,282 miles per second). This is denoted by the symbol \"c\" in physics equations.\n"
     ]
    }
   ],
   "source": [
    "await ProcessPrompt(\"State the speed of light.\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
