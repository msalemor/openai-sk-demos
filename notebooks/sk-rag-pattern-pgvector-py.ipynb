{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python RAG Pattern with Semantic Kernel and PgVector\n",
    "\n",
    "## Azure PostgreSQL Flexible Server - PGVector Setup in Azure\n",
    "\n",
    "### Running a local database with a container:\n",
    "\n",
    "- docker pull pgvector/pgvector:pg16\n",
    "- Then execute:\n",
    "\n",
    "```bash\n",
    "docker run --name pgvector16 \\\n",
    "  --restart unless-stopped \\\n",
    "  -p 5432:5432 \\\n",
    "  -e POSTGRES_PASSWORD=password \\\n",
    "  -v pgdata:/var/lib/postgresql/data \\\n",
    "  -d pgvector/pgvector:pg16\n",
    "```\n",
    "\n",
    "After deployment, connect using psql and type: `CREATE EXTENSION vector;`\n",
    "\n",
    "```bash\n",
    "docker exec -it pgvector16 psql -U postgres\n",
    "```\n",
    "\n",
    "### Running a Flexible Server in Azure - Manual Instructions:\n",
    "\n",
    "- Create a Flexible server instance in the Azure Portal\n",
    "- After creation, navigate to the Server Parameters pane:\n",
    "  - Search for azure.extensions\n",
    "  - Check the `Vector` value\n",
    "  - Save the changes and wait for the server to deploy\n",
    "- After deployment, open the instance and navigate to the `Database` panel:\n",
    "  - Click `Connect` link on the Postgres database\n",
    "    - Using the Cloud Shell psql, active the vector extension by typing: `CREATE EXTENSION vector;`\n",
    "\n",
    "Connection string:\n",
    "- Docker: `PG_CONN_STR_PY=\"postgresql://<user>:<password>@<server>:5432/<database>\"`\n",
    "- Azure: `PG_CONN_STR_PY=\"postgresql://<USER>:<PASSWORD>@<NAME>.postgres.database.azure.com:5432/postgres\"`\n",
    "\n",
    "Useful commands:\n",
    "\n",
    "- `truncate table public.\"PYCollection\";`\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q semantic-kernel==1.9.0 python-dotenv psycopg[binary,pool] azure-search-documents azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.memory import SemanticTextMemory, VolatileMemoryStore\n",
    "from semantic_kernel.core_plugins import TextMemoryPlugin\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.postgres.postgres_memory_store import (\n",
    "    PostgresMemoryStore,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.azure_cognitive_search import AzureCognitiveSearchMemoryStore, AzureAISearchSettings\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings, OpenAIChatPromptExecutionSettings\n",
    "from semantic_kernel.prompt_template import InputVariable, PromptTemplateConfig\n",
    "from semantic_kernel.functions import KernelArguments\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "COLLECTION_NAME = \"PYCollection\"\n",
    "ADA_EMBEDDINGS_SIZE = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "endpoint = os.getenv(\"GPT_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"GPT_OPENAI_KEY\")\n",
    "gpt_deployment_name = os.getenv(\"GPT_OPENAI_DEPLOYMENT_NAME\")\n",
    "conn_str = os.getenv(\"PG_CONN_STR_PY\")\n",
    "ada_deployment_name = \"text-embedding-ada-002\"\n",
    "ai_search_endpoint = os.getenv(\"AI_SEARCH_ENDPOINT\")\n",
    "ai_search_key = os.getenv(\"AI_SEARCH_KEY\")\n",
    "#print(endpoint, api_key, gpt_deployment_name, conn_str, ai_search_endpoint, ai_search_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a kernel instance configured for text completions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "kernel.add_service(AzureChatCompletion(\"gpt\",deployment_name=gpt_deployment_name, endpoint=endpoint, api_key=api_key))\n",
    "embedding_generator = AzureTextEmbedding(\"ada\",deployment_name=ada_deployment_name, endpoint=endpoint, api_key=api_key)\n",
    "kernel.add_service(embedding_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mem_store=VolatileMemoryStore()\n",
    "#mem_store = PostgresMemoryStore(conn_str,ADA_EMBEDDINGS_SIZE,1,3)\n",
    "mem_store = AzureCognitiveSearchMemoryStore(vector_size=ADA_EMBEDDINGS_SIZE,\n",
    "                                            search_endpoint=ai_search_endpoint,\n",
    "                                            admin_key=ai_search_key) \n",
    "if await mem_store.does_collection_exist(COLLECTION_NAME):\n",
    "    await mem_store.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "# async with AzureCognitiveSearchMemoryStore(vector_size=ADA_EMBEDDINGS_SIZE,search_endpoint=ai_search_endpoint,admin_key=ai_search_key) as acs_connector:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SemanticTextMemory(storage=mem_store, embeddings_generator=embedding_generator)\n",
    "kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n",
    "print(\"Kernel is ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "### Read the files and chunk them by paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file: str)->str:\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def ingest_content(path:str):\n",
    "    import os\n",
    "    chunks = []\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        if f.endswith(\"water.txt\"):            \n",
    "            content = read_file(\"data/\"+f)\n",
    "            paragraphs = content.split(\"\\n\\n\")\n",
    "            l = len(paragraphs)\n",
    "            id = 1\n",
    "            for p in paragraphs:\n",
    "                lid = f\"{f}-{l}-{id}\"\n",
    "                c = {\"id\":lid,\"chunk\":p,\"file\":f}\n",
    "                chunks.append(c)\n",
    "                id += 1\n",
    "    return chunks\n",
    "\n",
    "chunks = ingest_content(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the chunks and embeddings in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def populate_memory(memory: SemanticTextMemory, chunks: list) -> None:\n",
    "    for chunk in chunks:\n",
    "        await memory.save_information(collection=COLLECTION_NAME, id=chunk[\"id\"], text=chunk[\"chunk\"], description=chunk[\"file\"])\n",
    "\n",
    "await populate_memory(memory, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding\n",
    "\n",
    "### Find memories based on query, and collect the text in the memories to augment the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_memory_examples(memory, question: str, limit: int=3, relevance=0.75) -> list:\n",
    "    results = await memory.search(COLLECTION_NAME, question,limit,relevance)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Build a context from the text chunks in the memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the chemical composition of water?\"\n",
    "results = await search_memory_examples(memory, question)\n",
    "prompt_context = \"Context: \\\"\\\"\\\"\\n\"\n",
    "\n",
    "for result in results:\n",
    "    prompt_context += f\"Text:\\n{result.text}\\nSource:\\n{result.description}\\n\"\n",
    "    \n",
    "prompt_context += \"\\\"\\\"\\\"\"\n",
    "prompt_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Prompt & Completion\n",
    "\n",
    "### Create a SK function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt = \"\"\"\n",
    "{{$input}}\n",
    "\n",
    "{{ $context }}\n",
    "\"\"\".strip()\n",
    "\n",
    "arguments = KernelArguments(input=question, context=prompt_context)\n",
    "execution_settings = AzureChatPromptExecutionSettings(\n",
    "        service_id=\"gpt\",\n",
    "        max_tokens=50,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "answer = await kernel.invoke_prompt(rag_prompt,arguments=arguments,service_id=\"gpt\",execution_settings=execution_settings)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v31012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
