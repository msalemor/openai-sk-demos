{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python RAG Pattern with Semantic Kernel and PgVector\n",
    "\n",
    "## Azure PostgreSQL Flexible Server - PGVector Setup in Azure\n",
    "\n",
    "### Running a local database with a container:\n",
    "\n",
    "- docker pull pgvector/pgvector:pg16\n",
    "- Then execute:\n",
    "\n",
    "```bash\n",
    "docker run --name pgvector16 \\\n",
    "  --restart unless-stopped \\\n",
    "  -p 5432:5432 \\\n",
    "  -v pgdata:/var/lib/postgresql/data \\\n",
    "  -d pgvector/pgvector:pg16\n",
    "```\n",
    "- After deployment, connect using psql and type: `CREATE EXTENSION vector;`\n",
    "\n",
    "### Running a Flexible Server in Azure - Manual Instructions:\n",
    "\n",
    "- Create a Flexible server instance in the Azure Portal\n",
    "- After creation, navigate to the Server Parameters pane:\n",
    "  - Search for azure.extensions\n",
    "  - Check the `Vector` value\n",
    "  - Save the changes and wait for the server to deploy\n",
    "- After deployment, open the instance and navigate to the `Database` panel:\n",
    "  - Click `Connect` link on the Postgres database\n",
    "    - Using the Cloud Shell psql, active the vector extension by typing: `CREATE EXTENSION vector;`\n",
    "\n",
    "Connection string:\n",
    "- `PG_CONN_STR_PY=\"postgresql://<user>:<password>@<server>:5432/<database>\"`\n",
    "\n",
    "Useful commands:\n",
    "\n",
    "- `truncate table public.\"PYCollection\";`\n",
    "\n",
    "## Setup\n",
    "\n",
    "### Load required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import (\n",
    "    AzureChatCompletion,\n",
    "    AzureTextEmbedding,\n",
    ")\n",
    "from semantic_kernel.connectors.memory.postgres.postgres_memory_store import (\n",
    "    PostgresMemoryStore,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "COLLECTION_NAME = \"PYCollection\"\n",
    "ADA_EMBEDDINGS_SIZE = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "endpoint = os.getenv(\"GPT_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"GPT_OPENAI_KEY\")\n",
    "gpt_deployment_name = os.getenv(\"GPT_OPENAI_DEPLOYMENT_NAME\")\n",
    "conn_str = os.getenv(\"PG_CONN_STR_PY\")\n",
    "ada_deployment_name = \"ada\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a kernel instance configured for text completions and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "azure_chat_service = AzureChatCompletion(deployment_name=gpt_deployment_name, endpoint=endpoint, api_key=api_key)\n",
    "azure_text_embedding = AzureTextEmbedding(deployment_name=ada_deployment_name, endpoint=endpoint, api_key=api_key)\n",
    "kernel.add_chat_service(\"chat_completion\", azure_chat_service)\n",
    "kernel.add_text_embedding_generation_service(\"ada\", azure_text_embedding)\n",
    "\n",
    "mem_store = PostgresMemoryStore(conn_str,ADA_EMBEDDINGS_SIZE,1,3)\n",
    "if await mem_store.does_collection_exist(COLLECTION_NAME):\n",
    "    await mem_store.delete_collection(COLLECTION_NAME)\n",
    "    \n",
    "kernel.register_memory_store(memory_store=mem_store)\n",
    "kernel.import_plugin(sk.core_plugins.TextMemoryPlugin(), \"text_memory\")\n",
    "print(\"Kernel is ready to use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion\n",
    "\n",
    "### Read the files and chunk them by paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file: str)->str:\n",
    "    with open(file, \"r\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "def ingest_content(path:str):\n",
    "    import os\n",
    "    chunks = []\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        if f.endswith(\"water.txt\"):            \n",
    "            content = read_file(\"data/\"+f)\n",
    "            paragraphs = content.split(\"\\n\\n\")\n",
    "            l = len(paragraphs)\n",
    "            id = 1\n",
    "            for p in paragraphs:\n",
    "                lid = f\"{f}-{l}-{id}\"\n",
    "                c = {\"id\":lid,\"chunk\":p,\"file\":f}\n",
    "                chunks.append(c)\n",
    "                id += 1\n",
    "    return chunks\n",
    "\n",
    "chunks = ingest_content(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the chunks and embeddings in the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def populate_memory(kernel: sk.Kernel, chunks: list) -> None:\n",
    "    for chunk in chunks:\n",
    "        await kernel.memory.save_information(collection=COLLECTION_NAME, id=chunk[\"id\"], text=chunk[\"chunk\"], description=chunk[\"file\"])\n",
    "\n",
    "await populate_memory(kernel, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding\n",
    "\n",
    "### Find memories based on query, and collect the text in the memories to augment the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_memory_examples(kernel: sk.Kernel, question: str, limit: int=3, relevance=0.75) -> list:\n",
    "    results = await kernel.memory.search(COLLECTION_NAME, question,limit,relevance)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Build a context from the text chunks in the memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the chemical composition of water?\"\n",
    "results = await search_memory_examples(kernel, question)\n",
    "prompt_context = \"Context: \\\"\\\"\\\"\\n\"\n",
    "\n",
    "for result in results:\n",
    "    prompt_context += f\"Text:\\n{result.text}\\nSource:\\n{result.description}\\n\"\n",
    "    \n",
    "prompt_context += \"\\\"\\\"\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Prompt & Completion\n",
    "\n",
    "### Create a SK function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate = \"{{$input}}\\n\\nContext: ===\\n{{$context}}\\n===\\nAdd a source reference to the end of each sentence. e.g. Apple is a fruit [reference1.pdf][reference2.pdf]. Use only the provided text.\"\n",
    "rag_function = kernel.create_semantic_function(prompt_template=promptTemplate, max_tokens=500, temperature=0.3)\n",
    "\n",
    "skf_context = kernel.create_new_context()\n",
    "skf_context[\"input\"] = question\n",
    "skf_context[\"context\"] = prompt_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Prompt and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await rag_function(context=skf_context)\n",
    "print(f\"user:\\n{question}\\n\\nassistant:\\n{result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v31012",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
